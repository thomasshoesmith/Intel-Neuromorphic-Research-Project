{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mature-massage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import lava.lib.dl.slayer as slayer\n",
    "\n",
    "from gsc_dataset import GSCDataset\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expressed-juice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f25624d1b50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(4205)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-strength",
   "metadata": {},
   "source": [
    "# Event sparsity loss\n",
    "\n",
    "Sparsity loss to penalize the network for high event-rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nonprofit-charger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_rate_loss(x, max_rate=0.01):\n",
    "    mean_event_rate = torch.mean(torch.abs(x))\n",
    "    return F.mse_loss(F.relu(mean_event_rate - max_rate), torch.zeros_like(mean_event_rate))\n",
    "\n",
    "def loss(output, target):\n",
    "    return F.mse_loss(output, target.to(\"cpu\")) #TODO: CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-corrections",
   "metadata": {},
   "source": [
    "# Network description\n",
    "\n",
    "__SLAYER 2.0__ (__`lava.dl.slayer`__) provides a variety of learnable _neuron models_ <!-- (`slayer.neuron.{cuba, rf, ad_lif, __sigma_delta__, ...}`)  --> , _synapses_ <!-- (`slayer.{synapse, complex.synapse}.{dense, conv, pool, convT, unpool}`)  --> _axons_ and _dendrites_ that support quantized training. \n",
    "For easier use, it also provides __`block`__ interface which packages the associated neurons, synapses, axons and dendrite features into a single module. \n",
    "\n",
    "__Sigma-delta blocks__ are available as `slayer.blocks.sigma_delta.{Dense, Conv, Pool, Input, Output, Flatten, ...}` which can be easily composed to create a variety of sequential network descriptions as shown below. The blocks can easily enable _synaptic weight normalization_, _neuron normalization_ as well as provide useful _gradient monitoring_ utility and _hdf5 network export_ utility.\n",
    "\n",
    "<!-- TODO:\n",
    "- Describe how easy it is to describe a network in slayer2.0\n",
    "- Parameter Quantization is automatically handled unless disabled\n",
    "- Weight and neuron normalization\n",
    "- gradient monitoring utility\n",
    "- hdf5 export utility -->\n",
    "\n",
    "These blocks can be used to create a network using standard PyTorch procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mature-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.count_calc = True\n",
    "        cuba_params = {\n",
    "                'threshold'    : 1.25, \n",
    "                'current_decay': 0.25, \n",
    "                'voltage_decay': 0.25, \n",
    "                'tau_grad'     : 0.1,\n",
    "                'scale_grad'   : 0.8,\n",
    "                'shared_param' : False, \n",
    "                'requires_grad': False, \n",
    "                'graded_spike' : False,\n",
    "            }\n",
    "\n",
    "        recurr_weight_scale = 1.0\n",
    "\n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                # layer 1\n",
    "                slayer.block.cuba.Dense(cuba_params, 80, 512, weight_scale=recurr_weight_scale),\n",
    "                slayer.block.cuba.Dense(cuba_params, 512, 35, weight_scale=1.0)\n",
    "                #slayer.block.cuba.Average(num_outputs=35) #TODO 35\n",
    "            ])\n",
    "\n",
    "        \n",
    "    def forward(self, spike):\n",
    "        count = []\n",
    "\n",
    "        for block in self.blocks:\n",
    "            # print(block)\n",
    "            # print(f'{block=}')\n",
    "            spike = block(spike)\n",
    "            # print(\"spike computed\")\n",
    "            # output, net_loss, count\n",
    "            #return torch.mean(spike, dim=-1),\n",
    "\n",
    "            event_cost += event_rate_loss(spike)\n",
    "            count.append(torch.sum(spike[..., 1:]).to(spike.dtype).item())\n",
    "\n",
    "        return spike, event_cost, torch.FloatTensor(count).reshape((1, -1)).to(spike.device)\n",
    "    \n",
    "\n",
    "    def grad_flow(self, path):\n",
    "        # helps monitor the gradient flow\n",
    "        grad = [b.synapse.grad_norm for b in self.blocks if hasattr(b, 'synapse')]\n",
    "\n",
    "        plt.figure()\n",
    "        plt.semilogy(grad)\n",
    "        plt.savefig(path + 'gradFlow.png')\n",
    "        plt.close()\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def export_hdf5(self, filename):\n",
    "        # network export to hdf5 format\n",
    "        h = h5py.File(filename, 'w')\n",
    "        layer = h.create_group('layer')\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            b.export_hdf5(layer.create_group(f'{i}'))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-willow",
   "metadata": {},
   "source": [
    "# Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brown-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch  = 8  # batch size\n",
    "lr     = 0.001 # leaerning rate\n",
    "lam    = 0.01  # lagrangian for event rate loss\n",
    "epochs = 20  # training epochs\n",
    "steps  = [60, 120, 160] # learning rate reduction milestones\n",
    "\n",
    "trained_folder = 'Trained'\n",
    "logs_folder = 'Logs'\n",
    "\n",
    "os.makedirs(trained_folder, exist_ok=True)\n",
    "os.makedirs(logs_folder   , exist_ok=True)\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a88e226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntraining_set = GSCDataset(\\n    train=True, \\n    transform=transforms.Compose([\\n        transforms.ToTensor(),\\n    ]), \\n)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datasets\n",
    "\"\"\"\n",
    "training_set = GSCDataset(\n",
    "    train=True, \n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]), \n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-spring",
   "metadata": {},
   "source": [
    "# Instantiate Network, Optimizer, Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exposed-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network().to(device)\n",
    "\n",
    "optimizer = torch.optim.RAdam(net.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "\"\"\"\n",
    "# Datasets\n",
    "training_set = GSCDataset(\n",
    "    train=True, \n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]), \n",
    ")\n",
    "testing_set = GSCDataset(\n",
    "    train=False, \n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "#train_loader = DataLoader(dataset=training_set, batch_size=batch, shuffle=True, num_workers=8)\n",
    "#test_loader  = DataLoader(dataset=testing_set , batch_size=batch, shuffle=True, num_workers=8)\n",
    "\n",
    "stats = slayer.utils.LearningStats()\n",
    "assistant = slayer.utils.Assistant(\n",
    "        net=net,\n",
    "        error=lambda output, target: F.mse_loss(output.flatten(), target.flatten()),\n",
    "        optimizer=optimizer,\n",
    "        stats=stats,\n",
    "        count_log=True,\n",
    "        lam=lam\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a0d4e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!! validation dataset loaded successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'training_images = np.expand_dims(training_images, 1)\\ntesting_images = np.expand_dims(testing_images, 1)\\nvalidation_images = np.expand_dims(validation_images, 1)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "x_train = np.load(os.path.expanduser(\"/homes/ts468/data/rawSC/rawSC_80input/\") + \"training_x_data.npy\")\n",
    "y_train = np.load(os.path.expanduser(\"/homes/ts468/data/rawSC/rawSC_80input/\") + \"training_y_data.npy\")\n",
    "\n",
    "x_test = np.load(os.path.expanduser(\"/homes/ts468/data/rawSC/rawSC_80input/\") + \"testing_x_data.npy\")\n",
    "y_test = np.load(os.path.expanduser(\"/homes/ts468/data/rawSC/rawSC_80input/\") + \"testing_y_data.npy\")\n",
    "\n",
    "training_images = x_train #np.swapaxes(x_train, 1, 2) \n",
    "testing_images = x_test #np.swapaxes(x_test, 1, 2) \n",
    "\n",
    "training_images = training_images + abs(np.floor(training_images.min()))\n",
    "testing_images = testing_images + abs(np.floor(testing_images.min()))\n",
    "\n",
    "training_labels = y_train\n",
    "testing_labels = y_test\n",
    "\n",
    "# adding validation data if exists\n",
    "validation_images = np.array([])\n",
    "validation_labels = np.array([])\n",
    "if os.path.isfile(os.path.expanduser(\"/homes/ts468/data/rawSC/rawSC_80input/\") + \"validation_y_data.npy\"):\n",
    "        print(\"!! validation dataset loaded successfully\")\n",
    "        x_validation = np.load(os.path.expanduser(\"/homes/ts468/data/rawSC/rawSC_80input/\") + \"validation_x_data.npy\")\n",
    "        y_validation = np.load(os.path.expanduser(\"/homes/ts468/data/rawSC/rawSC_80input/\") + \"validation_y_data.npy\")\n",
    "\n",
    "        validation_images = x_validation #np.swapaxes(x_validation, 1, 2) \n",
    "        validation_images = validation_images + abs(np.floor(validation_images.min()))\n",
    "\n",
    "        validation_labels = y_validation\n",
    "\n",
    "\"\"\"training_images = np.expand_dims(training_images, 1)\n",
    "testing_images = np.expand_dims(testing_images, 1)\n",
    "validation_images = np.expand_dims(validation_images, 1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da5ca991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94824, 80, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3252df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "training_set = GSCDataset(\n",
    "    data = training_images, \n",
    "    targets = training_labels)\n",
    "\n",
    "testing_set = GSCDataset(\n",
    "data = training_images, \n",
    "targets = training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1628d75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# random index, tuple of x,y\n",
    "training_set.__getitem__(0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3facf6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=training_set, batch_size=batch, shuffle=True)\n",
    "test_loader  = DataLoader(dataset=testing_set , batch_size=batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a12cce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac9fd6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_dataloader = DataLoader(TensorDataset(torch.Tensor(training_images),\\n                                               torch.Tensor(training_labels)),\\n                                batch_size=32,\\n                                shuffle=True)\\n\\ntesting_dataloader = DataLoader(TensorDataset(torch.Tensor(testing_images),\\n                                              torch.Tensor(testing_labels)),\\n                                batch_size=32,\\n                                shuffle=True)\\n\\nvalidation_dataloader = DataLoader(TensorDataset(torch.Tensor(validation_images),\\n                                                 torch.Tensor(validation_labels)),\\n                                    batch_size=32,\\n                                    shuffle=True)\\n\\n#train_loader = DataLoader(dataset=training_set, batch_size=32, shuffle=True)\\n#test_loader  = DataLoader(dataset=testing_set , batch_size=32, shuffle=True)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"training_dataloader = DataLoader(TensorDataset(torch.Tensor(training_images),\n",
    "                                               torch.Tensor(training_labels)),\n",
    "                                batch_size=32,\n",
    "                                shuffle=True)\n",
    "\n",
    "testing_dataloader = DataLoader(TensorDataset(torch.Tensor(testing_images),\n",
    "                                              torch.Tensor(testing_labels)),\n",
    "                                batch_size=32,\n",
    "                                shuffle=True)\n",
    "\n",
    "validation_dataloader = DataLoader(TensorDataset(torch.Tensor(validation_images),\n",
    "                                                 torch.Tensor(validation_labels)),\n",
    "                                    batch_size=32,\n",
    "                                    shuffle=True)\n",
    "\n",
    "#train_loader = DataLoader(dataset=training_set, batch_size=32, shuffle=True)\n",
    "#test_loader  = DataLoader(dataset=testing_set , batch_size=32, shuffle=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ede8ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_loader = training_dataloader\\ntest_loader = testing_dataloader'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"train_loader = training_dataloader\n",
    "test_loader = testing_dataloader\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed71e292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11853"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-comfort",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "\n",
    "Training loop mainly consists of looping over epochs and calling `assistant.train` and `assistant.test` utilities over training and testing dataset. The `assistant` utility takes care of statndard backpropagation procedure internally.\n",
    "\n",
    "* `stats` can be used in print statement to get formatted stats printout.\n",
    "* `stats.testing.best_loss` can be used to find out if the current iteration has the best testing loss. Here, we use it to save the best model.\n",
    "* `stats.update()` updates the stats collected for the epoch.\n",
    "* `stats.save` saves the stats in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "antique-combining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 80, 100]) torch.Size([8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2266660/3754004120.py:27: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([280])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  error=lambda output, target: F.mse_loss(output.flatten(), target.flatten()),\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (280) must match the size of tensor b (8) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (\u001b[38;5;28minput\u001b[39m, ground_truth) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader): \u001b[38;5;66;03m# training loop\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape, ground_truth\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 9\u001b[0m     \u001b[43massistant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m3d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (\u001b[38;5;28minput\u001b[39m, ground_truth) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader): \u001b[38;5;66;03m# testing loop\u001b[39;00m\n",
      "File \u001b[0;32m~/lava_env/lib/python3.8/site-packages/lava/lib/dl/slayer/utils/assistant.py:125\u001b[0m, in \u001b[0;36mAssistant.train\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         output, net_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m--> 125\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(output, target)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#train_loader = DataLoader(dataset=training_set, batch_size=batch, shuffle=True, num_workers=8)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#test_loader  = DataLoader(dataset=testing_set , batch_size=batch, shuffle=True, num_workers=8)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m stats \u001b[38;5;241m=\u001b[39m slayer\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mLearningStats()\n\u001b[1;32m     25\u001b[0m assistant \u001b[38;5;241m=\u001b[39m slayer\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mAssistant(\n\u001b[1;32m     26\u001b[0m         net\u001b[38;5;241m=\u001b[39mnet,\n\u001b[0;32m---> 27\u001b[0m         error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m output, target: \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     28\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     29\u001b[0m         stats\u001b[38;5;241m=\u001b[39mstats,\n\u001b[1;32m     30\u001b[0m         count_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m         lam\u001b[38;5;241m=\u001b[39mlam\n\u001b[1;32m     32\u001b[0m     )\n",
      "File \u001b[0;32m~/lava_env/lib/python3.8/site-packages/torch/nn/functional.py:3294\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3292\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3294\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/lava_env/lib/python3.8/site-packages/torch/functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (280) must match the size of tensor b (8) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    if epoch in steps:\n",
    "        for param_group in optimizer.param_groups:    \n",
    "            print('\\nLearning rate reduction from', param_group['lr'])\n",
    "            param_group['lr'] /= 10/3\n",
    "        \n",
    "    for i, (input, ground_truth) in enumerate(train_loader): # training loop\n",
    "        print(input.shape, ground_truth.shape)\n",
    "        assistant.train(input, ground_truth)\n",
    "        print(f'\\r[Epoch {epoch:3d}/{epochs}] {stats}', end='')\n",
    "    \n",
    "    for i, (input, ground_truth) in enumerate(test_loader): # testing loop\n",
    "        assistant.test(input, ground_truth)\n",
    "        print(f'\\r[Epoch {epoch:3d}/{epochs}] {stats}', end='')\n",
    "        \n",
    "    if epoch%50==49: print() \n",
    "    if stats.testing.best_loss:  \n",
    "        torch.save(net.state_dict(), trained_folder + '/network.pt')\n",
    "    stats.update()\n",
    "    stats.save(trained_folder + '/')\n",
    "    \n",
    "    # gradient flow monitoring\n",
    "    net.grad_flow(trained_folder + '/')\n",
    "    \n",
    "    # checkpoint saves\n",
    "    if epoch%10 == 0:\n",
    "        torch.save({'net': net.state_dict(), 'optimizer': optimizer.state_dict()}, logs_folder + f'/checkpoint{epoch}.pt')                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-float",
   "metadata": {},
   "source": [
    "# Learning plots.\n",
    "\n",
    "Plotting the learning curves is as easy as calling `stats.plot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.plot(figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-monday",
   "metadata": {},
   "source": [
    "# Export the best trained model\n",
    "\n",
    "Load the best model during training and export it as hdf5 network. It is supported by `lava.lib.dl.netx` to automatically load the network as a lava process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(trained_folder + '/network.pt'))\n",
    "net.export_hdf5(trained_folder + '/network.net')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
