{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, n_recc_neur: List[int]=[256, 256], \n",
    "                n_channels: int=512, \n",
    "                recurr_weight_scale: float=1.0,\n",
    "                quantize: str='default'):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        cuba_params = {\n",
    "                'threshold'    : 1.25, \n",
    "                'current_decay': 0.25, \n",
    "                'voltage_decay': 0.25, \n",
    "                'tau_grad'     : 0.1,\n",
    "                'scale_grad'   : 0.8,\n",
    "                'shared_param' : False, \n",
    "                'requires_grad': False, \n",
    "                'graded_spike' : False,\n",
    "            }\n",
    "\n",
    "\n",
    "        # recurrent\n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                    slayer.block.cuba.Recurrent(cuba_params, n_channels, n_recc_neur[0], weight_scale=recurr_weight_scale, pre_hook_fx=None) if quantize == 'None' else \n",
    "                    slayer.block.cuba.Recurrent(cuba_params, n_channels, n_recc_neur[0], weight_scale=recurr_weight_scale),\n",
    "                ]\n",
    "                +\n",
    "                [slayer.block.cuba.Recurrent(cuba_params, n_recc_neur[i-1], n_recc_neur[i], weight_scale=recurr_weight_scale, pre_hook_fx=None) if quantize == 'None' else \n",
    "                slayer.block.cuba.Recurrent(cuba_params, n_recc_neur[i-1], n_recc_neur[i], weight_scale=recurr_weight_scale)\n",
    "                    for i in range(1, len(n_recc_neur))]\n",
    "                +\n",
    "                [ \n",
    "                slayer.block.cuba.Dense(cuba_params, n_recc_neur[-1], 288, weight_scale=2, pre_hook_fx=None) if quantize == 'None' else \n",
    "                slayer.block.cuba.Dense(cuba_params, n_recc_neur[-1], 288, weight_scale=2)\n",
    "                ,\n",
    "                slayer.block.cuba.Average(num_outputs=12)\n",
    "            ])\n",
    "        # feedforward\n",
    "\n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                slayer.block.cuba.Dense(cuba_params, n_channels, n_recc_neur[0], weight_scale=recurr_weight_scale, pre_hook_fx=None) if quantize == 'None' else \n",
    "                slayer.block.cuba.Dense(cuba_params, n_channels, n_recc_neur[0], weight_scale=recurr_weight_scale),\n",
    "                ]\n",
    "                +\n",
    "                [slayer.block.cuba.Dense(cuba_params, n_recc_neur[i-1], n_recc_neur[i], weight_scale=recurr_weight_scale, pre_hook_fx=None) if quantize == 'None' else \n",
    "                slayer.block.cuba.Dense(cuba_params, n_recc_neur[i-1], n_recc_neur[i], weight_scale=recurr_weight_scale)\n",
    "                    for i in range(1, len(n_recc_neur))]\n",
    "                +\n",
    "                [\n",
    "                slayer.block.cuba.Dense(cuba_params, n_recc_neur[-1], 288, weight_scale=2),\n",
    "                slayer.block.cuba.Average(num_outputs=12) #TODO 35\n",
    "            ])\n",
    "        # norse recurrent\n",
    "\n",
    "\n",
    "\n",
    "        class mean_across_time(torch.nn.Module):\n",
    "            def forward(self, x):\n",
    "                return torch.mean(x, dim=0)\n",
    "\n",
    "        class mean_across_groups_of_neurons(torch.nn.Module):\n",
    "            def __init__(self, n_out: int=12):\n",
    "                self.n_out = n_out\n",
    "                super().__init__()\n",
    "\n",
    "            def forward(self, x):\n",
    "                new_shape = (*x.shape[:-1], self.n_out, int(x.shape[-1] / self.n_out))\n",
    "                return x.reshape(*new_shape).mean(dim=-1)\n",
    "\n",
    "        self.model = norse.torch.SequentialState(\n",
    "            norse.torch.LIFRecurrent(n_channels, n_recc_neur[0], dt=norse_param_dict['dt'], p=LIF_parameters),\n",
    "            *[norse.torch.LIFRecurrent( n_recc_neur[i], n_recc_neur[i+1], dt=norse_param_dict['dt'], p=LIF_parameters) for i in range(len(n_recc_neur)-1)],\n",
    "            torch.nn.Linear(n_recc_neur[-1], 288),\n",
    "            norse.torch.LIF(dt=norse_param_dict['dt'], p=LIF_parameters),\n",
    "            mean_across_time(),\n",
    "            mean_across_groups_of_neurons(12),\n",
    "        )\n",
    "\n",
    "        # norse feedforwadr\n",
    "\n",
    "        class mean_across_time(torch.nn.Module):\n",
    "            def forward(self, x):\n",
    "                return torch.mean(x, dim=0)\n",
    "\n",
    "        class mean_across_groups_of_neurons(torch.nn.Module):\n",
    "            def __init__(self, n_out: int=12):\n",
    "                self.n_out = n_out\n",
    "                super().__init__()\n",
    "\n",
    "            def forward(self, x):\n",
    "                new_shape = (*x.shape[:-1], self.n_out, int(x.shape[-1] / self.n_out))\n",
    "                return x.reshape(*new_shape).mean(dim=-1)\n",
    "        self.model = norse.torch.SequentialState(\n",
    "            torch.nn.Linear(n_channels, n_recc_neur[0]),\n",
    "            norse.torch.LIF(dt=norse_param_dict['dt'], p=LIF_parameters),\n",
    "            *list(itertools.chain(\n",
    "                *[\n",
    "                [torch.nn.Linear(n_recc_neur[i], n_recc_neur[i+1]),\n",
    "            norse.torch.LIF(dt=norse_param_dict['dt'], p=LIF_parameters)] for i in range(len(n_recc_neur)-1)\n",
    "            ])),\n",
    "            torch.nn.Linear(n_recc_neur[-1], 288),\n",
    "            norse.torch.LIF(dt=norse_param_dict['dt'], p=LIF_parameters),\n",
    "            mean_across_time(),\n",
    "            mean_across_groups_of_neurons(12),\n",
    "        ) \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
