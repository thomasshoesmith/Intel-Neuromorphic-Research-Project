{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_PATH=/usr/local/cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export CUDA_PATH=/usr/local/cuda\n",
    "# sudo apt-get install python3-tk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from ml_genn import InputLayer, Layer, SequentialNetwork\n",
    "from ml_genn.callbacks import Checkpoint, SpikeRecorder, VarRecorder, Callback\n",
    "from ml_genn.compilers import EventPropCompiler, InferenceCompiler\n",
    "from ml_genn.connectivity import Dense\n",
    "from ml_genn.initializers import Normal\n",
    "from ml_genn.neurons import LeakyIntegrate, LeakyIntegrateFire, SpikeInput, LeakyIntegrateFireInput\n",
    "from ml_genn.optimisers import Adam\n",
    "from ml_genn.serialisers import Numpy\n",
    "from ml_genn.synapses import Exponential\n",
    "from time import perf_counter\n",
    "\n",
    "from ml_genn.utils.data import (calc_latest_spike_time, linear_latency_encode_data)\n",
    "\n",
    "from ml_genn.compilers.event_prop_compiler import default_params\n",
    "\n",
    "import random\n",
    "import librosa\n",
    "\n",
    "# constants\n",
    "params = {}\n",
    "params[\"NUM_INPUT\"] = 40\n",
    "params[\"NUM_HIDDEN\"] = 256\n",
    "params[\"NUM_OUTPUT\"] = 20\n",
    "params[\"BATCH_SIZE\"] = 128\n",
    "params[\"INPUT_FRAME_TIMESTEP\"] = 2\n",
    "params[\"INPUT_SCALE\"] = 0.008\n",
    "params[\"NUM_EPOCH\"] = 1\n",
    "params[\"NUM_FRAMES\"] = 80\n",
    "params[\"verbose\"] = True\n",
    "params[\"lr\"] = 0.01\n",
    "\n",
    "#weights\n",
    "params[\"hidden_w_mean\"] = 0.0 #0.5\n",
    "params[\"hidden_w_sd\"] = 3.5 #4.0\n",
    "params[\"output_w_mean\"] = 3.0 #0.5\n",
    "params[\"output_w_sd\"] = 1.5 #1\n",
    "\n",
    "file_path = os.path.expanduser(\"~/data/rawHD/experimental_2/\")#\"/its/home/ts468/data/rawHD/experimental_2/\"#\"/home/ts468/Documents/data/rawHD/experimental_2/\"\n",
    "return_accuracy = False\n",
    "\n",
    "\n",
    "\n",
    "# change dir for readout files\n",
    "try:\n",
    "    os.mkdir(\"HD_eventprop_cross_validation_output\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "os.chdir(\"HD_eventprop_cross_validation_output\")\n",
    "\n",
    "# Load testing data\n",
    "x_train = np.load(file_path + \"training_x_data.npy\")\n",
    "y_train = np.load(file_path + \"training_y_data.npy\")\n",
    "\n",
    "x_test = np.load(file_path + \"testing_x_data.npy\")\n",
    "y_test = np.load(file_path + \"testing_y_data.npy\")\n",
    "\n",
    "training_details = pd.read_csv(file_path + \"training_details.csv\")\n",
    "testing_details = pd.read_csv(file_path + \"testing_details.csv\")\n",
    "\n",
    "training_images = np.swapaxes(x_train, 1, 2) \n",
    "testing_images = np.swapaxes(x_test, 1, 2) \n",
    "\n",
    "training_images = training_images + abs(np.floor(training_images.min()))\n",
    "testing_images = testing_images + abs(np.floor(testing_images.min()))\n",
    "\n",
    "training_labels = y_train\n",
    "testing_labels = y_test\n",
    "\n",
    "if params.get(\"verbose\"): print(training_details.head())\n",
    "speaker_id = np.sort(training_details.Speaker.unique())\n",
    "if params.get(\"verbose\"): print(np.sort(training_details.Speaker.unique()))\n",
    "\n",
    "speaker = list(training_details.loc[:, \"Speaker\"])\n",
    "\n",
    "# readout class\n",
    "class CSVTrainLog(Callback):\n",
    "    def __init__(self, filename, output_pop, resume):\n",
    "        # Create CSV writer\n",
    "        self.file = open(filename, \"a\" if resume else \"w\")\n",
    "        self.csv_writer = csv.writer(self.file, delimiter=\",\")\n",
    "\n",
    "        # Write header row if we're not resuming from an existing training run\n",
    "        if not resume:\n",
    "            self.csv_writer.writerow([\"Epoch\", \"Num trials\", \"Number correct\", \"accuracy\", \"Time\"])\n",
    "\n",
    "        self.output_pop = output_pop\n",
    "\n",
    "    def on_epoch_begin(self, epoch):\n",
    "        self.start_time = perf_counter()\n",
    "\n",
    "    def on_epoch_end(self, epoch, metrics):\n",
    "        m = metrics[self.output_pop]\n",
    "        self.csv_writer.writerow([epoch, \n",
    "                                m.total, \n",
    "                                m.correct,\n",
    "                                m.correct / m.total,\n",
    "                                perf_counter() - self.start_time])\n",
    "        self.file.flush()\n",
    "\n",
    "# Create sequential model\n",
    "serialisers = []\n",
    "\n",
    "for s in speaker_id:\n",
    "    serialisers.append(Numpy(f\"serialiser_{s}\"))\n",
    "    \n",
    "network = SequentialNetwork(default_params)\n",
    "with network:\n",
    "    # Populations\n",
    "    input = InputLayer(LeakyIntegrateFireInput(v_thresh=4, #1\n",
    "                                            tau_mem=10, #20\n",
    "                                            input_frames=params.get(\"NUM_FRAMES\"), \n",
    "                                            input_frame_timesteps=params.get(\"INPUT_FRAME_TIMESTEP\")),\n",
    "                        params.get(\"NUM_INPUT\"), \n",
    "                        record_spikes = True)\n",
    "    \n",
    "    hidden = Layer(Dense(Normal(mean = params.get(\"hidden_w_mean\"),\n",
    "                                sd = params.get(\"hidden_w_sd\"))), \n",
    "                LeakyIntegrateFire(v_thresh=5.0, #1\n",
    "                                    tau_mem=20.0,\n",
    "                                    tau_refrac=None),\n",
    "                params.get(\"NUM_HIDDEN\"), \n",
    "                Exponential(5.0), #5\n",
    "                record_spikes=True)\n",
    "    \n",
    "    output = Layer(Dense(Normal(mean = params.get(\"output_w_mean\"),\n",
    "                                sd = params.get(\"output_w_sd\"))),\n",
    "                LeakyIntegrate(tau_mem=20.0, \n",
    "                                readout=\"avg_var\"),\n",
    "                params.get(\"NUM_OUTPUT\"), \n",
    "                Exponential(5.0), #5\n",
    "                record_spikes=True)\n",
    "    \n",
    "compiler = EventPropCompiler(example_timesteps = params.get(\"NUM_FRAMES\") * params.get(\"INPUT_FRAME_TIMESTEP\"),\n",
    "                        losses=\"sparse_categorical_crossentropy\",\n",
    "                        optimiser=Adam(params.get(\"lr\")), batch_size = params.get(\"BATCH_SIZE\"),\n",
    "                        reg_lambda_lower = 1e-9,\n",
    "                        reg_lambda_upper = 1e-9,\n",
    "                        reg_nu_upper= 20,\n",
    "                        kernel_profiling=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_net = compiler.compile(network)\n",
    "    \n",
    "for count, speaker_left in enumerate(speaker_id):\n",
    "    train= np.where(speaker != speaker_left)[0]\n",
    "    evalu= np.where(speaker == speaker_left)[0]\n",
    "    train_spikes= np.array([ training_images[i] for i in train ])\n",
    "    eval_spikes= np.array([ training_images[i] for i in evalu ])\n",
    "    train_labels= [ training_labels[i] for i in train ]\n",
    "    eval_labels= [ training_labels[i] for i in evalu ]\n",
    "    \n",
    "    print(f\"speaker {speaker_left} of {len(speaker_id)}\")\n",
    "    print(\"\\ncount\", count)\n",
    "\n",
    "    with compiled_net:\n",
    "        # Evaluate model on numpy dataset\n",
    "        if return_accuracy:\n",
    "            callbacks = [Checkpoint(serialisers[count])]\n",
    "        else:\n",
    "            callbacks = [\"batch_progress_bar\", \n",
    "                        Checkpoint(serialisers[count]), \n",
    "                        CSVTrainLog(f\"train_output_{speaker_left}.csv\", \n",
    "                                    output,\n",
    "                                    False),\n",
    "                        SpikeRecorder(input, key=\"input_spikes\")]\n",
    "        \"\"\"                \n",
    "        metrics = compiled_net.train({input: train_spikes * params.get(\"INPUT_SCALE\")},\n",
    "                                        {output: train_labels},\n",
    "                                        num_epochs=, \n",
    "                                        shuffle=True,\n",
    "                                        callbacks=callbacks,\n",
    "                                        validation_x= {input: eval_spikes * params.get(\"INPUT_SCALE\")},\n",
    "                                        validation_y= {output: eval_labels})\n",
    "        \"\"\"\n",
    "        callbacks = [\"batch_progress_bar\", Checkpoint(serialisers[count]),\n",
    "                     SpikeRecorder(hidden, key=\"hidden_spikes\")]\n",
    "        metrics, train_cb_data  = compiled_net.train({input: train_spikes * params.get(\"INPUT_SCALE\")},\n",
    "                                        {output: train_labels},\n",
    "                                        num_epochs=params.get(\"NUM_EPOCH\"),\n",
    "                                        callbacks=callbacks,\n",
    "                                        shuffle=True,\n",
    "                                        validation_x= {input: eval_spikes * params.get(\"INPUT_SCALE\")},\n",
    "                                        validation_y= {output: eval_labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_cb_data[\"hidden_spikes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle serialisers\n",
    "with open('serialisers.pkl', 'wb') as f:\n",
    "    pickle.dump(serialisers, f)\n",
    "    \n",
    "# evaluate\n",
    "\n",
    "network.load((params.get(\"NUM_EPOCH\") - 1,), serialisers[len(speaker_id) - 1])\n",
    "\n",
    "compiler = InferenceCompiler(evaluate_timesteps = params.get(\"NUM_FRAMES\") * params.get(\"INPUT_FRAME_TIMESTEP\"),\n",
    "                            reset_in_syn_between_batches=True,\n",
    "                            batch_size = params.get(\"BATCH_SIZE\"))\n",
    "compiled_net = compiler.compile(network)\n",
    "\n",
    "with compiled_net:\n",
    "    if return_accuracy:\n",
    "        callbacks = [Checkpoint(serialisers[11])]\n",
    "    else:\n",
    "        callbacks = [\"batch_progress_bar\", \n",
    "                    SpikeRecorder(input, key=\"input_spikes\"), \n",
    "                    SpikeRecorder(hidden, key=\"hidden_spikes\"),\n",
    "                    SpikeRecorder(output, key=\"output_spikes\"),\n",
    "                    VarRecorder(output, \"v\", key=\"v_output\")]\n",
    "\n",
    "    metrics, cb_data = compiled_net.evaluate({input: training_images * params.get(\"INPUT_SCALE\")},\n",
    "                                            {output: training_labels},\n",
    "                                            callbacks = callbacks)\n",
    "    \n",
    "if params.get(\"verbose\") and not return_accuracy:\n",
    "    # cannot print verbose whilst requesting just accuracy\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "    fig.suptitle('rawHD with EventProp on ml_genn')\n",
    "\n",
    "    value = random.randint(0, len(x_test))\n",
    "\n",
    "    ax1.scatter(cb_data[\"hidden_spikes\"][0][value], \n",
    "                cb_data[\"hidden_spikes\"][1][value], s=1)\n",
    "    ax1.set_xlabel(\"Time [ms]\")\n",
    "    ax1.set_ylabel(\"Neuron ID\")\n",
    "    ax1.set_title(\"Hidden\")\n",
    "    ax1.set_xlim(0, params.get(\"NUM_FRAMES\") * params.get(\"INPUT_FRAME_TIMESTEP\"))\n",
    "    ax1.set_ylim(0, params.get(\"NUM_HIDDEN\"))\n",
    "\n",
    "    ax2.scatter(cb_data[\"input_spikes\"][0][value], \n",
    "                cb_data[\"input_spikes\"][1][value], s=1)\n",
    "    ax2.set_xlabel(\"Time [ms]\")\n",
    "    ax2.set_ylabel(\"Neuron ID\")\n",
    "    ax2.set_title(\"Input\")\n",
    "    ax2.set_xlim(0, params.get(\"NUM_FRAMES\") * params.get(\"INPUT_FRAME_TIMESTEP\"))\n",
    "    ax2.set_ylim(0, params.get(\"NUM_INPUT\"))\n",
    "\n",
    "    ax3.plot(cb_data[\"v_output\"][value])\n",
    "    ax3.set_xlabel(\"Time [ms]\")\n",
    "    ax3.set_ylabel(\"voltage (v)\")\n",
    "    ax3.set_title(\"Output voltage\")\n",
    "    ax3.set_xlim(0, params.get(\"NUM_FRAMES\") * params.get(\"INPUT_FRAME_TIMESTEP\"))\n",
    "    #ax3.set_ylim(0, params.get(\"NUM_INPUT\"))\n",
    "\n",
    "    sr = 22050\n",
    "    img = librosa.display.specshow(x_train[value], \n",
    "                            x_axis='time', \n",
    "                            y_axis='mel', \n",
    "                            sr=sr, \n",
    "                            cmap='viridis')\n",
    "    #fig.colorbar(img, ax = ax4)\n",
    "    ax4.set_title(\"mel encoding\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # show accuracy log\n",
    "    for speaker_left in speaker_id:\n",
    "\n",
    "        data = pd.read_csv(f\"train_output_{speaker_left}.csv\")\n",
    "        df = pd.DataFrame(data, columns=['accuracy'])\n",
    "\n",
    "        accuracy = np.array(df)\n",
    "\n",
    "        accuracy = accuracy * 100\n",
    "\n",
    "        validation = []\n",
    "        training = []\n",
    "\n",
    "        for i in range(len(accuracy)):\n",
    "            if i % 2 == 0:\n",
    "                training.append(float(accuracy[i]))\n",
    "            else:\n",
    "                validation.append(float(accuracy[i]))\n",
    "                \n",
    "                \n",
    "        plt.plot(training, label = f\"training_{speaker_left}\")\n",
    "        #plt.plot(validation, label = f\"validation_{speaker_left}\")\n",
    "    plt.ylabel(\"accuracy (%)\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.title(\"accuracy during training\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# reset directory\n",
    "os.chdir(\"..\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genn_4_8_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
