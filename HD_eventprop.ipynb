{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_PATH=/usr/local/cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export CUDA_PATH=/usr/local/cuda\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "import os\n",
    "\n",
    "from ml_genn import InputLayer, Layer, SequentialNetwork\n",
    "from ml_genn.callbacks import Checkpoint, SpikeRecorder, VarRecorder, Callback\n",
    "from ml_genn.compilers import EventPropCompiler, InferenceCompiler\n",
    "from ml_genn.connectivity import Dense\n",
    "from ml_genn.initializers import Normal\n",
    "from ml_genn.neurons import LeakyIntegrate, LeakyIntegrateFire, SpikeInput, LeakyIntegrateFireInput\n",
    "from ml_genn.optimisers import Adam\n",
    "from ml_genn.serialisers import Numpy\n",
    "from ml_genn.synapses import Exponential\n",
    "from time import perf_counter\n",
    "\n",
    "from ml_genn.utils.data import (calc_latest_spike_time, linear_latency_encode_data)\n",
    "\n",
    "from ml_genn.compilers.event_prop_compiler import default_params\n",
    "\n",
    "import random\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "params = {}\n",
    "params[\"NUM_INPUT\"] = 40\n",
    "params[\"NUM_HIDDEN\"] = 256\n",
    "params[\"NUM_OUTPUT\"] = 20\n",
    "params[\"BATCH_SIZE\"] = 128\n",
    "params[\"INPUT_FRAME_TIMESTEP\"] = 2\n",
    "params[\"INPUT_SCALE\"] = 0.008\n",
    "params[\"NUM_EPOCH\"] = 50\n",
    "params[\"NUM_FRAMES\"] = 80\n",
    "params[\"verbose\"] = True\n",
    "params[\"lr\"] = 0.01\n",
    "\n",
    "#weights\n",
    "params[\"hidden_w_mean\"] = 0.0 #0.5\n",
    "params[\"hidden_w_sd\"] = 3.5 #4.0\n",
    "params[\"output_w_mean\"] = 3.0 #0.5\n",
    "params[\"output_w_sd\"] = 1.5 #1\n",
    "\n",
    "file_path = \"/its/home/ts468/data/rawHD/experimental_2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dir for readout files\n",
    "try:\n",
    "    os.mkdir(\"HD_eventprop_output\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "os.chdir(\"HD_eventprop_output\")\n",
    "\n",
    "# Load testing data\n",
    "x_train = np.load(file_path + \"training_x_data.npy\")\n",
    "y_train = np.load(file_path + \"training_y_data.npy\")\n",
    "\n",
    "x_test = np.load(file_path + \"testing_x_data.npy\")\n",
    "y_test = np.load(file_path + \"testing_y_data.npy\")\n",
    "\n",
    "training_details = pd.read_csv(file_path + \"training_details.csv\")\n",
    "testing_details = pd.read_csv(file_path + \"testing_details.csv\")\n",
    "\n",
    "training_images = np.swapaxes(x_train, 1, 2) \n",
    "testing_images = np.swapaxes(x_test, 1, 2) \n",
    "\n",
    "training_images = training_images + abs(np.floor(training_images.min()))\n",
    "testing_images = testing_images + abs(np.floor(testing_images.min()))\n",
    "\n",
    "training_labels = y_train\n",
    "testing_labels = y_test\n",
    "\n",
    "if params.get(\"verbose\"): print(testing_details.head())\n",
    "speaker_id = np.sort(testing_details.Speaker.unique())\n",
    "if params.get(\"verbose\"): print(np.sort(testing_details.Speaker.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# readout class\n",
    "\n",
    "class CSVTrainLog(Callback):\n",
    "    def __init__(self, filename, output_pop, resume):\n",
    "        # Create CSV writer\n",
    "        self.file = open(filename, \"a\" if resume else \"w\")\n",
    "        self.csv_writer = csv.writer(self.file, delimiter=\",\")\n",
    "\n",
    "        # Write header row if we're not resuming from an existing training run\n",
    "        if not resume:\n",
    "            self.csv_writer.writerow([\"Epoch\", \"Num trials\", \"Number correct\", \"accuracy\", \"Time\"])\n",
    "\n",
    "        self.output_pop = output_pop\n",
    "\n",
    "    def on_epoch_begin(self, epoch):\n",
    "        self.start_time = perf_counter()\n",
    "\n",
    "    def on_epoch_end(self, epoch, metrics):\n",
    "        m = metrics[self.output_pop]\n",
    "        self.csv_writer.writerow([epoch, \n",
    "                                m.total, \n",
    "                                m.correct,\n",
    "                                m.correct / m.total,\n",
    "                                perf_counter() - self.start_time])\n",
    "        self.file.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequential model\n",
    "serialiser = Numpy(\"latency_hd_checkpoints\")\n",
    "network = SequentialNetwork(default_params)\n",
    "with network:\n",
    "    # Populations\n",
    "    input = InputLayer(LeakyIntegrateFireInput(v_thresh=4,\n",
    "                                            tau_mem=10, \n",
    "                                            input_frames=params.get(\"NUM_FRAMES\"), \n",
    "                                            input_frame_timesteps=params.get(\"INPUT_FRAME_TIMESTEP\")),\n",
    "                        params.get(\"NUM_INPUT\"), \n",
    "                        record_spikes = True)\n",
    "    \n",
    "    hidden = Layer(Dense(Normal(mean = params.get(\"hidden_w_mean\"), # m = .5, sd = 4 ~ 68%\n",
    "                                sd = params.get(\"hidden_w_sd\"))), \n",
    "                LeakyIntegrateFire(v_thresh=5.0, \n",
    "                                    tau_mem=20.0,\n",
    "                                    tau_refrac=None),\n",
    "                params.get(\"NUM_HIDDEN\"), \n",
    "                Exponential(5.0), #5\n",
    "                record_spikes=True)\n",
    "    \n",
    "    output = Layer(Dense(Normal(mean = params.get(\"output_w_mean\"), # m = 0.5, sd = 1 @ ~ 66\n",
    "                                sd = params.get(\"output_w_sd\"))),\n",
    "                LeakyIntegrate(tau_mem=20.0, \n",
    "                                readout=\"avg_var\"),\n",
    "                params.get(\"NUM_OUTPUT\"), \n",
    "                Exponential(5.0), #5\n",
    "                record_spikes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler = EventPropCompiler(example_timesteps = params.get(\"NUM_FRAMES\") * params.get(\"INPUT_FRAME_TIMESTEP\"),\n",
    "                        losses=\"sparse_categorical_crossentropy\",\n",
    "                        optimiser=Adam(params.get(\"lr\")), \n",
    "                        batch_size = params.get(\"BATCH_SIZE\"),\n",
    "                        reg_lambda_lower = 1e-6, #1e-6\n",
    "                        reg_lambda_upper = 1e-6,\n",
    "                        reg_nu_upper= 2,\n",
    "                        dt = 1)\n",
    "\n",
    "\n",
    "compiled_net = compiler.compile(network)\n",
    "\n",
    "with compiled_net:\n",
    "    # Evaluate model on numpy dataset\n",
    "    start_epoch = 0\n",
    "    callbacks = [\"batch_progress_bar\", \n",
    "                Checkpoint(serialiser), \n",
    "                CSVTrainLog(\"train_output.csv\", \n",
    "                            output,\n",
    "                            False),\n",
    "                SpikeRecorder(hidden, key = \"hidden_spike_counts\", record_counts = True)]\n",
    "        \n",
    "    metrics, metrics_val, cb_data_training, cb_data_validation = compiled_net.train({input: training_images * params.get(\"INPUT_SCALE\")},\n",
    "                                    {output: training_labels},\n",
    "                                    num_epochs = params.get(\"NUM_EPOCH\"), \n",
    "                                    shuffle=True,\n",
    "                                    validation_split = 0.1,\n",
    "                                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cb_data_training[\"hidden_spike_counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cb_data_training[\"hidden_spike_counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hidden_spike_counts.npy', 'wb') as f:\n",
    "    np.save(f, cb_data_training[\"hidden_spike_counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "\n",
    "network.load((params.get(\"NUM_EPOCH\") - 1,), serialiser)\n",
    "\n",
    "compiler = InferenceCompiler(evaluate_timesteps = params.get(\"NUM_FRAMES\") * params.get(\"INPUT_FRAME_TIMESTEP\"),\n",
    "                            reset_in_syn_between_batches=True,\n",
    "                            batch_size = params.get(\"BATCH_SIZE\"),\n",
    "                            reg_lambda_lower = 1e-9,\n",
    "                            reg_lambda_upper = 1e-9,\n",
    "                            reg_nu_upper= 20)\n",
    "\n",
    "compiled_net = compiler.compile(network)\n",
    "\n",
    "with compiled_net:\n",
    "    callbacks = [\"batch_progress_bar\", \n",
    "                SpikeRecorder(input, key=\"input_spikes\"), \n",
    "                SpikeRecorder(hidden, key=\"hidden_spikes\"),\n",
    "                SpikeRecorder(output, key=\"output_spikes\"),\n",
    "                VarRecorder(output, \"v\", key=\"v_output\")]\n",
    "\n",
    "    metrics, cb_data = compiled_net.evaluate({input: training_images * params.get(\"INPUT_SCALE\")},\n",
    "                                            {output: training_labels},\n",
    "                                            callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cb_data[\"hidden_spikes\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.get(\"verbose\"):\n",
    "    # cannot print verbose whilst requesting just accuracy\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "    fig.suptitle('rawHD with EventProp on ml_genn')\n",
    "\n",
    "    value = random.randint(0, len(x_test))\n",
    "\n",
    "    ax1.scatter(cb_data[\"hidden_spikes\"][0][value], \n",
    "                cb_data[\"hidden_spikes\"][1][value], s=1)\n",
    "    ax1.set_xlabel(\"Time [ms]\")\n",
    "    ax1.set_ylabel(\"Neuron ID\")\n",
    "    ax1.set_title(\"Hidden\")\n",
    "    ax1.set_xlim(0, params.get(\"NUM_FRAMES\") * params.get(\"INPUT_FRAME_TIMESTEP\"))\n",
    "    ax1.set_ylim(0, params.get(\"NUM_INPUT\"))\n",
    "\n",
    "    ax2.scatter(cb_data[\"input_spikes\"][0][value], \n",
    "                cb_data[\"input_spikes\"][1][value], s=1)\n",
    "    ax2.set_xlabel(\"Time [ms]\")\n",
    "    ax2.set_ylabel(\"Neuron ID\")\n",
    "    ax2.set_title(\"Input\")\n",
    "    ax2.set_xlim(0, params.get(\"NUM_FRAMES\") * params.get(\"INPUT_FRAME_TIMESTEP\"))\n",
    "    ax2.set_ylim(0, params.get(\"NUM_INPUT\"))\n",
    "\n",
    "    ax3.plot(cb_data[\"v_output\"][value])\n",
    "    ax3.set_xlabel(\"Time [ms]\")\n",
    "    ax3.set_ylabel(\"voltage (v)\")\n",
    "    ax3.set_title(\"Output voltage\")\n",
    "    ax3.set_xlim(0, params.get(\"NUM_FRAMES\") * params.get(\"INPUT_FRAME_TIMESTEP\"))\n",
    "    #ax3.set_ylim(0, params.get(\"NUM_INPUT\"))\n",
    "\n",
    "    sr = 22050\n",
    "    img = librosa.display.specshow(x_train[value], #use imshow instead\n",
    "                            x_axis='time', \n",
    "                            y_axis='mel', \n",
    "                            sr=sr, \n",
    "                            cmap='viridis')\n",
    "    #fig.colorbar(img, ax = ax4)\n",
    "    ax4.set_title(\"mel encoding\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    data = pd.read_csv(\"train_output.csv\")\n",
    "    df = pd.DataFrame(data, columns=['accuracy'])\n",
    "\n",
    "    accuracy = np.array(df)\n",
    "\n",
    "    accuracy = accuracy * 100\n",
    "\n",
    "    validation = []\n",
    "    training = []\n",
    "\n",
    "    for i in range(len(accuracy)):\n",
    "        if i % 2 == 0:\n",
    "            training.append(float(accuracy[i]))\n",
    "        else:\n",
    "            validation.append(float(accuracy[i]))\n",
    "                        \n",
    "    plt.plot(training, label = \"training\")\n",
    "    plt.plot(validation, label = \"validation\")\n",
    "    plt.ylabel(\"accuracy (%)\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.title(\"accuracy during training\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# reset directory\n",
    "\n",
    "os.chdir(\"..\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genn_4_8_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
